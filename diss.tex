% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout

\usepackage[utf8]{inputenc} 
% allows inclusion of PDF, PNG and JPG images
\usepackage{graphicx}
\graphicspath{ {figs/}}

% For plots
\newlength\figureheight
\newlength\figurewidth
\usepackage{pgfplots}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm} %Used for the indicator function
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;} %specifies the argmax command

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{siunitx} %For better alignement of numbers in tables. 

\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex

\usepackage{multirow}
\usepackage{hhline} % Draw partial double lines in tables

\usepackage{listings} % Package to display code and customize highlighting
\usepackage{color}

\usepackage[list=true,listformat=simple]{subcaption} % subfigures
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{python}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
%\lstset{style=mystyle} % Set code listings style

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Sebastian Borgeaud dit Avocat}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Brain tumour segmentation using Convolutional Neural Networks} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Fitzwilliam College\\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

%TC:ignore
\chapter*{Proforma}

{\large
\begin{tabular}{lp{12cm}}
Name:               & \bf Sebastian Borgeaud dit Avocat                       \\
College:            & \bf Fitzwilliam College                     \\
Project Title:      & \bf Brain tumour segmentation using Convolutional Neural Networks\\
Examination:        & \bf Computer Science Tripos -- Part II, June 2017  \\
Word Count:         & \bf 11,965\footnotemark[1]
  \\
Project Originator: & Duo Wang                    \\
Supervisor:         & Dr. Mateja Jamnik \& Duo Wang                    \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{texcount -sum -sub=chapter diss.tex}
}
\stepcounter{footnote}
\section*{Original aims of the project}The main aim for this project was to implement an algorithm based on a convolutional neural network that performs brain tumour segmentation. The second aim was to achieve similar results to those obtained by Pereira et al.\ \cite{pereira}, the current \mbox{state-of-the-art}. In particular, the success criteria was to achieve 90\% of the accuracy reported by Pereira, thus, obtaining Dice scores of (0.79, 0.73, 0.67) for the regions `complete' (including classes 2–-4), ‘core’ (classes 3–-4) and ‘enhancing’ (class 4) in the BraTS2013 challenge \cite{brats-proceedings}, respectively.

\section*{Completed work}
The first goal has been achieved: Pereira's method was replicated to obtain a model that learns how to perform brain tumour segmentation. The Dice scores obtained are (0.8149, 0.6704, 0.5921), performing better in the first region than Pereira's model but not achieving the 90\% threshold for the two other regions. Then, a second model was designed using more recent techniques, achieving an extension goal. This second model obtained Dice scores of (0.75, 0.54, 0.53) and has the advantage of segmenting scans about 7 times faster than the model proposed by Pereira.

\section*{Special difficulties}
The main variant of Pereira's model used a semi-automatic normalisation which requires input from an expert. Therefore, a fully-automatic variant, also reported by Pereira et al.\ \cite{pereira} in the same paper, was implemented.
\newpage
\section*{Declaration}

I, Sebastian Borgeaud dit Avocat of Fitzwilliam College, being a candidate for Part II of the Computer Science Tripos, hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Date: \today}

\tableofcontents

{\listoffigures \let\cleardoublepage\clearpage \listoftables}


\newpage

\setlength{\parskip}{1em} %Add vertical space between paragraphs.

\section*{Acknowledgements}
I would like to thank my supervisors, Dr. Mateja Jamnik and Duo Wang, for proposing this project to me, and for patiently providing assistance and answering my questions throughout the project. Moreover, I would also like to thank the Computational Biology group in the Computer Lab for giving me access to one of their GPUs. Then, I would like to thank my Director of Studies, Dr. Robert Harle, who read an early version of this dissertation and provided useful feedback. Finally, I would also like to thank Sergio Pereira for sharing with me the parameters and heuristics he used, and for answering my questions regarding his model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

%TC:endignore

\pagestyle{headings}

\chapter{Introduction}
\section{Motivation}
In recent years, convolutional neural networks have been shown to significantly outperform other methods in many areas of Computer Science such as Computer Vision (FaceNet\cite{face_net}), image classification (AlexNet \cite{alex_net}), Natural language processing \cite{nlp_deep_learning} and several others. The field of bioinformatics and medical imaging is no exception to this, in particular, convolutional neural networks have been shown to perform as well as pervious \mbox{state-of-the-art} methods and even to outperform them on the problem of brain tumour segmentation \cite{pereira, kamnitas}.

Brain tumours may be primary or secondary. Primary brain tumours start in the brain while secondary (or metastatic) tumours spread to the brain from a different part of the body. Primary brain tumours are further divided into different types, of which the glioma is the most frequently occurring in adults. The  glioma arises from glial cells and infiltrates the surrounding tissue. Goodenberger and Jenkins \cite{gliomas_research} reported that gliomas make up to 30\% of all brain and central nervous system tumours and 80\% of all malignant brain tumours. These gliomas are further classified into low-grade and high-grade, depending on their pace of growth. While the low-grade gliomas come with a life expectancy of several years, the clinical population with the more aggressive form of the disease -- the high-grade glioma -- have a median survival rate of two years or less \cite{gliomas_life}. For both groups, intensive neuroimaging protocols are used both before and after the treatment to evaluate the progression of the disease and the success of the treatment. Image processing algorithms that can automatically analyse tumour scans would therefore be of great value for improved diagnosis, treatment planning and evaluation of the tumour progression. 

However, developing these automated brain tumour segmentation algorithms is technically challenging as lesion areas are only defined by changes in intensity relative to the surrounding normal tissue. Tumour size, extension and localisation also vary across patients, making it hard to incorporate and encode strong priors on shape and location, which are often used in segmentations of anatomical structures \cite{brats-proceedings}. The problem of automatic brain tumour segmentation is therefore still an active research area. 

The first aim of my project is to understand and to replicate one of the more recent techniques applying convolutional neural networks to the problem of brain tumour segmentation. I chose to replicate a method proposed by Pereira et al.\cite{pereira} in 2016 which is based on two-dimensional convolutional neural networks. 

In the second phase of my project, I introduce a novel network architecture that achieves similar performance as my implementation of Pereira's model. My model uses a larger receptive field, but is still able to segment scans much faster.

\section{Related work}
Brain tumour segmentation algorithms can be divided into generative and discriminative models.

Generative methods rely on domain-specific prior knowledge, and combine that knowledge with the appearance of a new scan to detect anomalies. They therefore often generalise well to previously unseen images. However, encoding such prior knowledge is very difficult. For example, Prastawa et al.\ \cite{prastawa} used a brain atlas to detect abnormal regions. A post-processing step is then applied to ensure that these tumour regions have good spatial regularity.

On the other hand, discriminative methods use very little prior knowledge on the brain's anatomy. After the extraction of low level image features using manually annotated images, discriminative models learn directly how to model the relationship between these features and the label of a given voxel. The features used vary across methods: Hamamci et al.\ \cite{hamamci} used raw input pixels, for example, and Kleesiek et al.\ \cite{kleesiek} used local histograms. The methods usually train a classifier that relies on these hand-designed features. These features are assumed to have a sufficiently high discriminative power so that the classifier can learn to separate the tissue into the appropriate classes. The problem with such \mbox{hand-designed} features is that by their nature they are generic, with no specific adaptation to the domain of brain tumours. Ideally, these low-level features should be composed into higher-level and more task-specific features. Convolutional neural networks do this to some extent; starting from the raw input data, they extract increasingly more complex features using those computed by the previous layer in the network.

Zikic et al.\ \cite{zikic} first proposed a convolutional neural network with two convolutional layers followed by a fully-connected layer for brain tumour segmentation. The inputs to the network were patches of size $19 \times 19$ taken from slices of the scans. Thus, only a two-dimensional convolutional neural network is required which is much more efficient than a three-dimensional one and is less prone to overfitting. Building on this approach, Havaei et al.\ \cite{havaei} used a novel double-pathway architecture and a cascade of two networks to rank second on the BraTS2015 challenge. Novel in their approach was also the training of the network in two phases, first sampling patches from an equiprobable distribution before training only the fully-connected layers on data sampled using proportions near the originals. Pereira \cite{pereira} proposed a deeper two-dimensional convolutional neural network of 11 layers, that ranked first in the BraTS2013 challenge. As the aim of my project is to replicate this method, the details are explained in the Preparation and Implementation chapters. 

Finally, it is worth noting that some of the more recent approaches have successfully overcome the difficulties involved in training three-dimensional neural networks. In particular, the model proposed by Kamnitsasa et al.\ \cite{kamnitas} used a three-dimensional convolutional neural network with residual connections \cite{resnet} to take the first place in the BraTS2015 challenge.

\section{Supervised learning and classification}
Machine learning tasks can be divided into supervised and unsupervised learning.
\begin{itemize}
	\item In unsupervised learning, the task is to learn a function to describe some hidden structure from unlabelled data. 
	\item In supervised learning, the task is to learn a relationship between data points from labelled data. More formally, the aim is to infer a function $f: A \to B$ from a set of $n$ labelled data points $\mathbb{X} = \{(x^{(1)}, y^{(1)}), ..., (x^{(n)}, y^{(n)})\}$, with $\mathbb{X} \subseteq A \times B$. Typically, if $A \subset \mathbb{R}^d$ and $B \subset \mathbb{R}$, then the task is also called \textbf{regression}. If $B = \{b_1, ..., b_k\}$ is discrete, the task is called \textbf{classification}. 
\end{itemize} 
In classification and regression, one approach to solve the task is by defining an appropriate loss function $\mathcal{L}: B \times B \to \mathbb{R}$. The loss function is then minimised over the labelled test dataset $\sum_{i=1}^n \mathcal{L}(f(\textbf{a}_i), b_i)$ in the hope that the learned function $f$ will be able to generalise well on unseen data.

Brain tumour segmentation is an instance of a supervised classification problem. The training data set $\mathbb{X}$ consists of MRI scans where each voxel is segmented into one of five classes:
\begin{enumerate}
	\setcounter{enumi}{-1}
	\item Non-tumour, which includes areas outside the brain and normal tissue inside the brain;
	\item Surrounding edema, which is swelling caused by excess fluid trapped in the brain's tissue;
	\item Necrotic tissue, which consists of dead cells;
	\item Non-enhancing tumour;
	\item Enhancing tumour.
\end{enumerate} 
An example of such a segmentation is shown in figure \ref{fig:example_segmentation}.

We can therefore reformulate the segmentation problem as the problem of learning a function $f$ that can map each pixel in the input scan to one of the 5 classes. If we then map this function $f$ to every voxel in an image, we have effectively created a segmentation.

\begin{figure}
	\centering
	\includegraphics[width = 0.5\textwidth]{challenge_1_segmentation_with_T2_66}
	\caption[Axial plane slice of an MRI scan overlaid by a possible segmentation.]{Axial plane slice of an MRI scan overlaid by a possible segmentation. The regions in grey, red, green, blue and yellow correspond to classes 0, 1, 2, 3 and 4 respectively. }
	\label{fig:example_segmentation}
\end{figure}

\section{Dissertation outline}
In the Preparation chapter I will explain how convolutional neural networks can be used to solve a supervised learning task. I will also give more details on how the data is structured and where it comes from. Then, in the Implementation chapter, I will outline  how I transform the data into a format that can be used to train a convolutional neural network, and how I implement the convolutional neural network proposed by Pereira \cite{pereira}. I will also present the architecture of my new model. Then, I will explain in more detail how I use the models to segment a scan efficiently. Finally, I will detail how I implement the post-processing step based on connected components. In the Evaluation chapter, I describe how segmentations can be evaluated quantitatively and will evaluate both models.

\chapter{Preparation}

\section{Starting point}
\subsection{Theory}
The starting point for this project was the part IB course `Artificial Intelligence I'. Neural networks, backpropagation and stochastic gradient descent were introduced, concepts also used in convolutional neural networks. The course also provided a brief introduction to machine learning and formalised the task of supervised learning.

The part II course 'Machine Learning and Bayesian Inference' had sections on the evaluation of classifiers and on general techniques for machine learning which were also useful.
%I was able to use some of the material taught by the part II course `Machine Learning and Bayesian Inference', especially those parts on the evaluation of classifiers and on general techniques for machine learning.

I learned the remaining theory through self study, mainly using the Stanford course `CS231n Convolutional Neural Networks for Visual Recognition'\footnote{\url{http://cs231n.github.io/}}.
\subsection{Programming languages \& libraries}
The project was almost entirely written in Python, except the computing jobs for the Cambridge High Performance Cluster, which are bash scripts. I had only used Python before for small projects and had to learn how Object Oriented Programming is handled in Python and some of the more advanced features. For manipulating the data, I used the Numpy\footnote{\url{http://www.numpy.org/}} library, which was also new to me.

To load in the scans, which are in MetaImage medical format (`.mha') I used the SimpleITK\footnote{\url{https://itk.org/Wiki/SimpleITK}} library for Python.

Finally, I used the Keras\footnote{\url{https://keras.io/}} library, which makes it easy to create and train convolutional neural networks while leaving all important design and architectural decisions to the user. 
%As Keras is becoming one of the standard open-source library in deep learning research and applications, many online resources were available when needed.

\subsection{Graphical processing unit (GPU)}
To train the convolutional neural network in a reasonable amount of time, I used the Wilkes Cluster\footnote{\url{https://www.hpc.cam.ac.uk/services/wilkes}} from the Cambridge University High Performance Computing services, which is equipped with NVIDIA K20 GPUs. From February 23\textsuperscript{rd} 2017, I was able to use the TITAN X GPUs from the Computational Biology group in the Computer Lab. This reduced the training time for the model proposed by Pereira from about 12 hours to 2--3 hours.

\section{Software engineering}
Training the model and segmenting scans can conceptually be divided into different stages. We can therefore model these two processes as two data pipelines, where each stage takes as an input the output of the previous stage. The pipeline for training the model and the pipeline for segmenting an MRI scan are shown respectively in Figures \ref{fig:training_implementation_pipeline} and \ref{fig:segmentation_implementation_pipeline}.

\begin{figure}[h]
	\captionsetup{justification=centering}
	\centering
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[width=\textwidth]{training_implementation_pipeline}
		\caption[Data pipeline corresponding to the training of the convolutional neural network.]{Data pipeline corresponding to the training of the convolutional neural network. \\The boxes in red describe the inputs and outputs of each stage.}
		\label{fig:training_implementation_pipeline}
	\end{subfigure}
	\qquad		
	
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[width=\textwidth]{segmentation_implementation_pipeline}
		\caption{Data pipeline corresponding to the segmentation of an MRI scan.}
		\label{fig:segmentation_implementation_pipeline}
	\end{subfigure}
	\caption{}
\end{figure}

I use a waterfall like approach for the implementation, building and testing the different stages independently. All stages except the scan normalisation and the post-processing are model dependent. Instead of reimplementing each stage twice, once for the model proposed by Pereira and once for the model I propose, I implement each stage only once in such a way that the model-dependent aspects can be specified by parameters. This flexibility is especially important as it makes rapid experimentation with many different model architectures possible.

It is important to note that division into sequential stages provides an effective way of modularising the code base, but it is mainly conceptual. In practice, due to the large amount of data, it cannot flow in this manner. For example, in the patch extraction phase for the segmentation, it would not be possible to load all patches in memory at once. The implementation details of these stages is described in the next chapter.

I used Git as a version control system as I was already familiar with it. The repository is stored locally and in a private GitHub repository. I pushed the contents of the local repository to GitHub after most commits. Furthermore, as an additional backup strategy I regularly backed up the local repository to my external hard disk.

\section{Theoretical background}
\subsection{Artificial neural networks}
To understand how convolutional neural networks work, we first need to be familiar with ordinary neural networks. These consist of a sequence of layers of neurons, in which each neuron has a set of trainable weights that can be adjusted to change the overall function computed by the neural network. An example architecture of a neural network is shown diagrammatically in Figure \ref{fig:nn_layout}. Each neuron in layer $n+1$ is connected to every neuron in layer $n$ and computes as an output
\begin{equation}
y = f_{act}((\sum_{i=1}^{n} y_i w_i) + b)	
\end{equation}
where $f_{act}$ is a non-linear, differentiable activation function, $y_i$ is the output of neuron $i$ in the previous layer, and the $w_i$ and $b$ are the weights of the neuron. A neuron is connected to every neuron in the previous layer, which is why this layer is also called a \textbf{fully-connected layer}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{nn_layout}
	\caption{Structure of a simple neural network with two hidden layers.}
	\label{fig:nn_layout}
\end{figure}

\subsubsection{Activation functions}
The most common activation functions are:
\begin{enumerate}
\item The Sigmoid function
\begin{equation}
S(x) = \frac{1}{1 + e^x} 
\end{equation}
\item The hyperbolic tangent
\begin{equation}
	\textrm{tanh}(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{equation}
\item The rectifier
\begin{equation}
\label{eq:linear_rectifier}
	f(x) = 
\begin{cases}
	0 & \text{if } x < 0\\
	x & \text{otherwise}
\end{cases}
\end{equation}
\item The leaky rectifier, with $0 < \alpha < 1$
\begin{equation}
f(x) = 
\begin{cases}
	\alpha x & \text{if } x < 0\\
	x & \text{otherwise}
\end{cases}
\end{equation}
\end{enumerate}
%the Sigmoid function:
%\begin{equation}
%S(x) = \frac{1}{1 + e^x} 
%\end{equation}
%the hyperbolic tangent:
%\begin{equation}
%	\textrm{tanh}(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}
%\end{equation}
%the rectifier:
%\begin{equation}
%\label{eq:linear_rectifier}
%	f(x) = 
%\begin{cases}
%	0 & \text{if } x < 0\\
%	x & \text{otherwise}
%\end{cases}
%\end{equation}
%and the leaky rectifier, for $0 < \alpha < 1$:
%\begin{equation}
%f(x) = 
%\begin{cases}
%	\alpha x & \text{if } x < 0\\
%	x & \text{otherwise}
%\end{cases}
%\end{equation}
These functions are plotted in figure \ref{fig:activation_functions}. 
\begin{figure}[h]
	\centering 
	\setlength\figureheight{10cm}
	\setlength\figurewidth{0.8\textwidth}
	\input{plots/activations_plot}
	\caption{Plot of the activation functions.}
	\label{fig:activation_functions}
\end{figure}

Historically, the hyperbolic tangent function or the sigmoid function were mainly used. However, the magnitude of the gradients of these two activation functions is always below 1. This causes a numerical issue for deeper networks, as the gradients for each layer are multiplied together during the backpropagation algorithm. This is known as the vanishing gradient problem \cite{vanishing_gradients} and is the reason why it is now preferred to use the rectifier or the leaky rectifier functions.

In a classification problem, the output layer usually consists of $K$ nodes, one for each class. Using a \textbf{softmax activation function} for the last layer, we can interpret the value at each node as the probability that the input belongs to this class.  More formally, in a network with parameters $\theta$, we can view the output of node $k$ when presented with input $x^{(i)}$ as the probability $P(y_{\text{pred}}^{(i)} = k \mid x^{(i)};\theta)$. The softmax activation computes for each output $k$
\begin{equation}
	\sigma(\mathbf{x})_k = \frac{e^{\mathbf{x}_k}}{\sum_{j=1}^{K}e^{\mathbf{x}_j}}
\end{equation}
where $\mathbf{x}$ is the vector consisting of all outputs from the previous layer.

\subsubsection{Loss function}
To train the network, we need to quantify how well it performs on the training data. A loss function is therefore introduced. Since the aim of the network is to classify the central pixel(s) of the patch, we use the \textbf{categorical cross-entropy loss} \cite[p.~137]{bishop} function
\begin{equation}
	\label{eq:loss}
	\mathcal{L}(\theta) = 
	- \frac{1}{m}\Big[\sum_{i=1}^m \sum_{k=1}^K\mathbbm{1}[y^{(i)} = k]\log(P(y_{\text{pred}}^{(i)}=k \mid x^{(i)};\theta))\Big]
\end{equation}
where $\mathbbm{1}$ is the indicator function and $P(y_{\text{pred}}^{(i)}=k \mid x^{(i)};\theta)$ is the probability computed by the neural network with weights $\theta$ that input vector $x^{(i)}$ with label $y^{(i)}$ belongs to class $k$.

\subsubsection{Optimisation}
We now aim to minimise this loss function. The gradient descent algorithm is used, which repeatedly updates the weights of the network using the gradient of the loss function:
\begin{equation}
	\theta_{i+1} = \theta_{i} - \epsilon \frac{d\mathcal{L}(\theta_{i})}{d\theta}
\end{equation}
where $\epsilon$ is the learning rate, a small positive value.

Since every basic operation used in the neural network is differentiable, the function computed by the network will also be differentiable, which in turn makes it possible to calculate the gradient of the loss function with respect to the weights in the network by repeatedly applying the chain rule. This process is called the \textbf{backpropagation algorithm}.

The loss functions, as described in equation \ref{eq:loss} is computed using the entire training data set $\mathbb{X} = \{(x^{(1)},y^{(1)}), ...,(x^{(m)},y^{(m)})\}$. In the case of deep learning, where it is usual to have a very large training data set, this would be very memory costly and slow down the training phase unnecessarily. A solution to this problem is to use \textbf{stochastic gradient descent} instead, where the training data is split into batches. The loss and gradient updates are then computed individually for each batch.

\subsubsection{Momentum}
The training process can be optimised using momentum update. Minimising the loss function can be interpreted as moving a small particle down a hilly terrain in the hyper-dimensional space defined by the loss function. Since the gradient is related to the force experienced by that particle, this suggests that the gradient should only influence the velocity vector and not the position directly: This leads to the velocity update
\begin{equation}
	v = \mu  v - \epsilon \frac{d\mathcal{L}(\theta)}{d\theta}
\end{equation}
where $\mu$ is the momentum. We then update our weights by simply adding the velocity to the current value.
\begin{equation}
	\theta = \theta + v
\end{equation}

Typically, a slightly different version called the Nesterov momentum is used, as it has been shown to work better in practice \cite{nesterov_momentum}.

\subsection{Convolutional neural networks}
Convolutional neural networks are based on the assumption that the inputs are images. This allows us to take advantage of properties of images to make the function computed by the network more effective and greatly reduce the number of weights in our network. 

A typical convolutional network constists of three types of layers:
\begin{enumerate}
	\item \textbf{Fully-connected layers}: These are identical to the layers in  neural networks introduced earlier.
	\item \textbf{Convolutional layers}: A convolutional layer defines a set of learnable three-dimensional filters. The width and height of the filters are typically smaller than the width and height of the input volume, but the depth of the filter has to equal the depth of the input volume. Each filter in the layer is convolved with the input and computes the dot product at each point, resulting in a two-dimensional activation map. These are then stacked to produce the three-dimensional output volume. The number of filters is defined by the depth of the layer as it determines the depth of the output volume.
	
	The computation done by a filter is shown in Figure \ref{fig:conv_example}. It has translational invariance as the filter is convolved with the entire input, thus detecting the same feature independently of location.  
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{conv_example2}
		\caption[Example computation done by a $3 \times 3$ filter in a convolutional layer.]{Example computation done by a $3 \times 3$ filter in a convolutional layer. The stride and the padding are 1, so as to keep the output dimensions identical. The output is computed by convolving the filter with the input volume, computing the dot product at each point.}
		\label{fig:conv_example}
	\end{figure}

	The size of the output volume is determined by the following hyperparameters:
	\begin{enumerate}
		\item The \textbf{depth} corresponds to the number of filters in the layer and is therefore equal to the depth of the output volume.
		\item The \textbf{stride} determines by how many pixels the filter is moved during each step of the convolution.
		\item The \textbf{padding} determines how many zeros are padded to the input width and height.
	\end{enumerate}
	
	\item \textbf{Pooling layers}: Pooling layers reduce the spatial size of the input layer, which decreases the number of parameters in the network. The most common pooling layer implementation is the \textbf{max-pooling} layer, which just convolves a two-dimensional maximum operator of a given size. The \textbf{stride} determines the step size.

\end{enumerate}

Typically, the first layers in a convolutional neural network consist of pairs of convolutional layers and max--pooling layers. The idea is that each  pair of layers can learn increasingly more abstract features using the features learned by the previous layers. For example, the first pair might learn to recognise edges, the second layer shapes, etc. The last few layers of the network consist entirely of fully-connected layers. These learn how to classify the data using the features learned by the convolutional and max pooling layers.

\subsection{The overfitting problem}
A common problem that arises with neural networks is \textbf{overfitting}. This occurs when the model is not able to generalise on previously unseen data and instead just memorises the training data. Many techniques and heuristics have been developed in order to help prevent this; in my project I have used three of them: \textbf{L2 weight regularisation}, \textbf{Dropout} and \textbf{Batch Normalisation}.

\subsubsection{\textbf{L2} weight regularisation}
The L2 regularisation prevents individual parameters from growing without bounds, instead making the network use all of its parameters, which has the effect of preventing the model from just memorising the data\footnote{\url{http://cs231n.github.io/neural-networks-2/\#reg}}. This is achieved by adding a regularisation penalty to the loss function, which is the sum of the squares of every parameter:
\begin{equation}
	R(\theta) = \sum_{k}^{} \sum_{l}^{} \theta_{k,l}^2
\end{equation} 
where $\theta$ is the matrix containing the weights of the network.
The loss function that we aim to minimise becomes
\begin{equation}
	\mathcal{L}(\theta) = \mathcal{L}_{data}(\theta) + \lambda R(\theta) 
\end{equation}
where $\mathcal{L}_{data}(\theta)$ is the data loss, defined in Equation \ref{eq:loss} and $\lambda$ is a hyperparameter weighting the regularisation penalty. 

\subsubsection{Dropout}
A more recent technique, developed for neural networks is Dropout \cite{dropout}. When Dropout is used, the network randomly drops some neurons during the training phase. This forces the network to distribute the computation on all neurons evenly, which prevents the network from overfitting. The algorithm is explained in more details in Appendix \ref{appendix:dropout}.

\subsubsection{Batch normalisation}
Batch normalisation \cite{batch_normalization} is an even more recent technique, which I use in the second phase of my project. The algorithm is applied on individual layers of the network, fixing the input distribution mean and variance for that layer. This allows the network to learn faster and makes it less prone to overfitting. The algorithm is explained in more details in Appendix \ref{appendix:batch_normalisation}.

\section{Data source}
\subsection{BraTS challenge}
For my project, I use the dataset provided by BraTS2013\cite{menze:hal-00935640} challenge. It is split into three parts:
\begin{enumerate}
	\item The training dataset consists of 30 patients and their ground truth marked by human experts. The patients are further divided into 20 high-grade glioma cases and 10 low-grade glioma cases. The difference between these two types of brain tumours is their rate  of growth, which is slower for the low-grade case.
	\item The challenge dataset consists of 10 high-grade patients without ground truth annotation. Participants of the challenge can segment these scans and submit their segmentation online for evaluation. 	
	\item The leaderboard dataset contains 25 patients, including both high-grade and low-grade gliomas. The ground truth labelling for these scans is not publicly available.
\end{enumerate}
The challenge and leaderboard sets are both used to rank the participants of the original BraTS conference. To create a benchmarking resource, an online platform was made available after the conference that evaluates and ranks submitted segmentations.

Each patient consists of 4 images taken using different MRI contrasts: T2 and FLAIR MRI, which highlight differences in tissue water relaxational properties, and T1 MRI and T1c MRI, which show pathological intratumoural take-up of contrast agents. Each of these modalities shows different type of biological information and may therefore be useful for creating different features during the classification of the tissues. The differences for the 4 modalities can be seen in Figure \ref{fig:mri_scans}. 

\begin{figure}
	\centering
	\includegraphics[scale=0.15]{T1_example}
	\includegraphics[scale=0.15]{T1c_example}
	\includegraphics[scale=0.15]{T2_example}			
	\includegraphics[scale=0.15]{Flair_example}
	\caption[Example slices in the axial plane for the 4 different scan modalities.]{Example slices in the axial plane for the 4 different scan modalities. From left to right the modalities are T1, T1c, T2 and Flair. The scan is from patient 1, slice $z=89$. The four modalities have some obvious differences which the convolutional neural network might be able to utilise to discriminate between tumour and non-tumour regions.}
	\label{fig:mri_scans}
\end{figure}

To homogenise the data across the different scans, each patient's image volumes are co-registered to the T1c MRI scan, which has the highest spatial resolution in most cases. Then, all images are resampled to 1mm isotropic resolution in a standardised axial orientation with linear interpolation. Finally, all images are skull stripped to guarantee the anonymity of the patients.

The training dataset has been manually segmented by 4 different human experts into the 5 classes described in the introduction. These segmentations are then merged together to provide a single ground truth segmentation. As an example of a segmentation, Figure \ref{fig:expert_segmentations} shows different slices of a T1 MRI scan annotated with the ground truth.

\begin{figure}
	\centering
	\includegraphics[scale=0.1]{expert_segmentation_49}
	\includegraphics[scale=0.1]{expert_segmentation_59}
	\includegraphics[scale=0.1]{expert_segmentation_69}
	\includegraphics[scale=0.1]{expert_segmentation_79}
	\includegraphics[scale=0.1]{expert_segmentation_89}
	\includegraphics[scale=0.1]{expert_segmentation_99}
	\includegraphics[scale=0.1]{expert_segmentation_109}
	\includegraphics[scale=0.1]{expert_segmentation_119}
	\includegraphics[scale=0.1]{expert_segmentation_129}
	\caption[Slices of a T1 MRI scan annotated with the expert labelling.]{Axial plane slices of a T1 MRI scan annotated by the expert labelling. The slices were taken from patient 1, using $z \in \{49, 59, 69, 79, 89, 99, 109, 119, 129\}$. The enhancing tumour region is in yellow, the non-enhancing tumour in blue, the edema in green and the necrotic core in red, corresponding to classes 4, 3, 2 and 1 respectively.}
	\label{fig:expert_segmentations}
\end{figure}



\subsection{High grade gliomas}
For this project I have decided to focus only on the high-grade glioma cases. I made this choice for the following reasons:
\begin{enumerate}
	\item To keep the scope of the project within that of a Part II project. Pereira proposed a different model for the low-grade glioma case which I would have had to replicate as well, possibly doubling the amount of work needed during the training phase.
	\item Secondly, the results of most research papers in this area are commonly reported in terms of high-grade glioma cases, as it is considered to be more challenging than the low-grade cases. 
	\item Lastly, the challenge dataset, which consists only of high-grade patients, allows me to easily compare the results of my models with those of other researchers. 
\end{enumerate}
Accordingly, I only use the 20 high-grade glioma cases of the training dataset to train the convolutional neural networks.

\chapter{Implementation}
This chapter is divided into two parts. The first part describes the implementation of the training phase of a model and follows the division into stages introduced in Figure \ref{fig:training_implementation_pipeline}. The second part describes how a trained model is used to segment MRI scans. The second part is divided into sections corresponding to the stages shown in \ref{fig:segmentation_implementation_pipeline}.

\section{Training a model}
The first step is to read in the scans, which is done using the SimpleITK library that has inbuilt support for the MetaImage medical (`.mha') format, in which the images are made available. For each patient, the 4 scans (T1, T1c, T2, Flair) are read in as numpy arrays and stacked along a new dimension, resulting in a 4-dimensional array for each patient of shape $(z_{\text{size}} \times y_{\text{size}} \times x_{\text{size}} \times 4)$.
 
As explained in the previous chapter the data consists of 20 different patients, each consisting of 4 different MRI scans, taken with different modalities. Unfortunately, the size of the different scans varies from patient to patient. Table \ref{table:scan_sizes} shows how the sizes are distributed among patients. Because of this, the data for the patients cannot be aggregated into a single 5-dimensional numpy array. Instead, I aggregate those arrays into a single python list containing all the training data.

\begin{table}[h]
\centering	
\begin{tabular}{ m{3.5cm} c } 
\textbf{Size ($z \times y \times z$)} & \textbf{Number of scans}\\
 \hline
 $\ 176 \times 216 \times 160$ & 8 \\ 
 $\ 176 \times 216 \times 176$ & 6 \\ 
 $\ 230 \times 230 \times 162$ & 2 \\ 
 $\ 176 \times 236 \times 216$ & 1 \\ 
 $\ 165 \times 230 \times 230$ & 1 \\ 
 $\ 240 \times 240 \times 168$ & 1 \\ 
 $\ 220 \times 220 \times 168$ & 1 \\ 
\end{tabular}
\caption[Scan sizes for the 20 different patients of the training dataset.]{Scan sizes for the 20 different high-grade glioma patients of the BraTS training dataset.}
\label{table:scan_sizes}
\end{table}

The model proposed by Pereira \cite{pereira} uses the Nyul normalisation \cite{nyul}. This normalisation requires human input, preferably from a domain expert. In order to make the process fully automatic, I use another method, also reported in Pereira's paper, described in the following section.

\subsection{Scan normalisation}
\label{section:scan_normalisations}
\subsubsection{Winsorising}
The first normalisation applied is called `winsorising'. The aim is to limit the values of extremes, in order to reduce the effect that outliers may have. This is also known as `clipping' in digital signal processing. I use a 98\% winsorisation, meaning that the data values below the 1\textsuperscript{st} percentile are set to the value of the 1\textsuperscript{st} percentile and the value above the 99\textsuperscript{th} percentile are set to the value of the 99\textsuperscript{th} percentile. Note that this process is different from trimming as the values are not discarded but just clipped.

\subsubsection{N4 Bias field correction}
The second normalisation applied to scans is the N4 bias field correction \cite{n4itk}. MRI scanners can create a bias in which the intensity of the same tissue varies across the produced scan, making it hard for automated segmentation algorithms to recognise that it is in fact the same tissue. N4 is a variant of the popular nonparametric nonuniform intensity normalisation N3, which aims at eliminating this bias introduced by MRI scanners. 

As this correction is rather complicated and not the main point of my project, I will not go into more details here. However, for completeness, table \ref{table:n4_params} lists the parameters I used to perform the normalisation. I first used the default parameters and later contacted Pereira who kindly agreed to share the parameters he used in his published method, which are reported here. I used the implementation provided by the `Nipype'\footnote{\url{http://nipype.readthedocs.io/en/latest/}} and the `Advanced Normalization Tools'\footnote{\url{http://stnava.github.io/ANTs/}}.

\begin{table}[h]
\centering	
\begin{tabular}{ c c } 
\textbf{Parameter} & \textbf{Value} \\
 \hline
 n\_iterations & [20,20,20,10] \\ 
 dimension & 3 \\
 bspline\_fitting\_distance & 200 \\
 shrink\_factor & 2\\
 convergence\_threshold & 0
\end{tabular}
\caption{Parameters used to perform the N4ITK correction.}
\label{table:n4_params}
\end{table}

Figure \ref{fig:n4itk_example} shows the effects that winsorising and then applying the N4 bias field correction has on scans for each of the four modalities T1, T1c, T2 and Flair.
\begin{figure}
	\centering
	\includegraphics[width=0.3\textwidth]{t1_winsorized_example}
	\includegraphics[width=0.3\textwidth]{t1_n4itk_example} \\
	\vspace{0.5cm}
	\includegraphics[width=0.3\textwidth]{t1c_winsorized_example}
	\includegraphics[width=0.3\textwidth]{t1c_n4itk_example} \\
	\vspace{0.5cm}
	\includegraphics[width=0.3\textwidth]{t2_winsorized_example}
	\includegraphics[width=0.3\textwidth]{t2_n4itk_example} \\
	\vspace{0.5cm}
	\includegraphics[width=0.3\textwidth]{flair_winsorized_example}
	\includegraphics[width=0.3\textwidth]{flair_n4itk_example}
	\caption[Example of the effects of winsorising and applying the N4ITK correction to a scan for each of the four modalities.]{Example of the effects of winsorising and applying the N4ITK correction to a scan for each of the four modalities. The first image in each row is obtained by winsorising the original image. The second image is obtained by applying the N4ITK correction to the winsorised scan. From top to bottom, slice $z=89$ is shown for patient 1 for the T1, T1c, T2 and Flair scans.}
	\label{fig:n4itk_example}
\end{figure}

\subsubsection{Mean and Variance standardisation}
The last step is to standardise the mean to 0 and the variance to 1 for each scan modality. This is done by first subtracting the mean value of the voxels in the slice and then dividing by the standard deviation of those values. 
\begin{equation}
	x' = \frac{x - \mu}{\sigma}
\end{equation}
As the true mean and standard deviation are not known, the sample mean and sample standard deviation must be used. 

I use numpy's \texttt{mean} and \texttt{std} functions, and the fact that basic operations on numpy arrays are done element-wise to implement this normalisation.

\subsection{Patch extraction}
\label{section:patch_extraction}
Each input to the convolutional neural network is a three-dimensional array, of shape $\textsf{height} \times \textsf{width} \times 4$, since it consists of a two-dimensional patch of $\textsf{width} \times \textsf{height}$ voxels for each one of the 4 scan modalities. The two-dimensional patches are taken along the x- -y axis, also called the axial plane in anatomy. Figure \ref{fig:patch_extraction} gives a diagrammatic overview of the transformation for the simplified case with a single modality, meaning that the image array is three-dimensional and the resulting patch is two-dimensional, instead of four and three-dimensional respectively.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{patch_extraction}
	\caption[Patch extraction process for a three dimensional array.]{Patch extraction process for a three dimensional array. The actual image array is four dimensional, the 4th dimension being the different modalities. Here $\textsf{width}=\textsf{height}=33$ and therefore corresponds to the patch size proposed by Pereira.}
	\label{fig:patch_extraction}
\end{figure}

There are two issues that need to be resolved before extracting the patches:
\begin{enumerate}
	\item The training data has to be balanced, that is, the same number of examples for each class should be included in the training data. This is to ensure that the convolutional model is able to generalise well. However, the BraTS dataset is extremely unbalanced with over 98\% of the voxels belonging to class 0, as shown in Table \ref{table:class_frequencies}. Therefore, simply randomly picking patches from the dataset does not work and a more complex method has to be used. Also note that a fixed balanced dataset would limit the maximum number of patches we can extract to 5 times the number of voxels in the least represented class. This is the non-enhancing tumour class (class 3) with 184,436 labeled voxels, hence a maximum of 922,130 patches. To increase the number of different patches the network sees during training for the overrepresented classes while keeping the data balanced, I randomly resample at each epoch the same number of patches for each class from all possible patches for that class.
		\begin{table}
			\centering	
			\begin{tabular}{c c S[table-format=9.0] S[table-format=2.2]}
			\textbf{Class} & \textbf{Tissue type} & \textbf{Number of labeled voxels} & \textbf{frequency}\\
			 \hline
			0 & Non tumour 				& 138958832 	& 98.23 \% \\ 
			1 & Necrosis 				& 282936 	& 0.20 \% \\ 
			2 & Edema					& 1466271 	& 1.36 \% \\ 
			3 & Non-enhancing tumour 	& 184436 	& 0.13 \% \\ 
			4 & Enhancing tumour		& 560777 	& 0.34 \% \\
			
			\end{tabular}
			\caption[Class frequencies for the 20 high-grade glioma patients of the BraTS2013 dataset.]{Class frequencies for the 20 high-grade glioma patients of the BraTS2013 dataset. Healthy tissue voxels (class 0) are highly overrepresented, which leads to issues when training the convolutional neural network. Therefore, patches have to be balanced during the patch extraction phase.}
			\label{table:class_frequencies}
		\end{table}
	\item It is not possible to extract an entire patch around the voxels too close to the edge of the scan. To solve this issue there are two options: I could either ignore the patch entirely or pad the scan such that each labelled voxel is surrounded by enough voxels. Only the first option is considered as it helps with the balanced classes issue, as most of the voxels close to an edge are from class 0. Furthermore, as the brains are centred within the MRI scans, voxels close to an edge are surrounded only by voxels of value 0, making the patch identical to many other patches close to an edge. Table \ref{table:valid_class_frequencies} shows the distribution of classes for ``valid'' voxels, that is, those at least more than half the patch length away from the x or y edges for Pereira's model with $\textsf{width}=\textsf{height}=33$. Indeed, most ignored voxels come from class 0.

		\begin{table}[h]
		\centering	
		\begin{tabular}{c c S[table-format=9.0] S[table-format=2.2] S[table-format=8.0]}
		\textbf{Class} & \textbf{Tissue type} & \textbf{Labelled voxels} & \textbf{Frequency} & \textbf{Ignored voxels}\\
		 \hline
		0 & Non tumour 				& 94773429 	& 97.45 \% & 44185403 \\ 
		1 & Necrosis 				& 281831 	& 0.29 \% & 1105\\ 
		2 & Edema					& 1453205 	& 1.49 \% & 13066\\ 
		3 & Non-enhancing tumour 	& 183396 	& 0.19 \% & 1040\\ 
		4 & Enhancing tumour		& 558320 	& 0.57 \% & 2457\\
		\end{tabular}
		\caption[Class frequencies for valid voxels in the 20 high-grade glioma patients of the BraTS2013 dataset.]{Class frequencies for valid voxels in the 20 high-grade glioma patients of the BraTS2013 dataset. A voxel is said to be valid if it is possible to extract a patch of size $33 \times 33$ around it. As most of the invalid voxels are in class 0, we can safely ignore invalid voxels.}
		\label{table:valid_class_frequencies}
		\end{table}
\end{enumerate}

In personal correspondence (see Appendix \ref{appendix:pereira_email}), Pereira mentioned that he used two further heuristics for extracting patches from class 0 that were not mentioned in his paper, which I also implemented:
\begin{enumerate}
	\item Half of the patches for class 0 are selected close to the tumour, where close means that the central voxel is within the bounding box of the diseased tissue. This helps the network to learn how to draw the boundaries between class 0 and the other classes.
	\item The other half of the training patches for class 0 are spaced apart with a distance of at least 3 voxels in the z, y and x directions. This spreads the input data to the entire scan.
\end{enumerate}


\subsubsection{Patch extraction algorithm}
To make the resampling of the patches at each epoch more efficient, I first create a list (\texttt{valid\_positions}) containing for each class the positions of the valid patches (really the position of the central voxel) for that class. Note that at this point it would be infeasible to store all possible patches, as it would require to store over a 100 billion 32 bit floats for class 0 alone, or over 400 GB. To get the indices of the valid voxels, I used numpy's \texttt{argwhere} function on the ground truth arrays. The \texttt{argwhere} function takes in an array and a predicate and returns the indices of those elements in the array satisfying the predicate. I use equality checking between the voxel label and the class number. This is done for every patient and returns an array consisting of the $(z, y, x)$ coordinates of valid patches for every patient. Since the patient number must also be included in the indices, I prepend the patient number along the first axis, resulting for each patient in an array consisting of the $(\text{patient}, z, y, x)$ coordinates and aggregate the results for each patient into a single list. The second step consists of removing those voxels that are too close to an x-axis or y-axis edge. Only indices $(\text{patient}, z, y, x)$ where the x and y values are  within the allowed ranges defined by half the size of the patch: $[\lfloor \frac{\textsf{width}}{2} \rfloor, \textsf{width}_{\text{scan}} - \lceil \frac{\textsf{width}}{2} \rceil]$, and similarly for the height, are kept. Again this can be done using numpy and filtering based on column values. Algorithm \ref{alg:patch_extraction} summarises the steps taken during the patch extraction. 

\begin{algorithm}
\caption{Patch extraction}\label{alg:patch_extraction}
\begin{algorithmic}[1]
\For{$\text{class } k \text{ in } [0,1,2,3,4]$}
	\State valid\_positions[$k$] $\gets$ []
	\For{\textbf{each} index \textbf{in} len(labels)}
		\State label $\gets$ labels[index] 
		\State possible\_indices $\gets$ \textit{numpy}.argwhere(label == $k$)
		\State patient\_indices $\gets$ \textit{numpy}.full(\textit{len}(possible\_indices), index)
		\State possible\_indices $\gets$ \textit{numpy}.append(patient\_indices, possible\_indices, axis = 1)
		\State
		\State possible\_indices $\gets$ possible\_indices[$\text{possible\_indices}[:,2] \ge \lfloor \frac{\textsf{height}}{2} \rfloor$]
		\State possible\_indices $\gets$ possible\_indices[$\text{possible\_indices}[:,2] \leq \text{height}_{\text{label}} - \lceil \frac{\textsf{height}}{2} \rceil$]
		\State possible\_indices $\gets$ possible\_indices[$\text{possible\_indices}[:,3] \ge \lfloor \frac{\textsf{width}}{2} \rfloor$]
		\State possible\_indices $\gets$ possible\_indices[$\text{possible\_indices}[:,3]  \leq  \text{width}_{\text{label}} - \lceil \frac{\textsf{width}}{2} \rceil$]
		\State
		\State valid\_indices[$k$] $\gets$ possible\_indices
	\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

At the beginning of each training epoch, I then randomly choose the same number of positions for each class from \texttt{valid\_positions} using numpy's \texttt{random.choice} function. For each position, I extract the corresponding patch and label which are returned as training data.

\subsection{Data augmentation}
To increase the amount of data available, data augmentation techniques can be used. In typical applications of convolutional neural networks for image processing and computer vision tasks, translations and rotations are used. In my case, the data consists entirely of two-dimensional patches, thus translation cannot be used as it would result in a different patch, with a possibly different label. However, using rotations of the patches might give some performance improvements. Pereira proposes the following variants:
\begin{enumerate}
	\item No rotations.
	\item Rotations of 90, 180 and 270 degrees.
	\item Uniformly sample three rotations from an array of equally spaced angles. The angle step proposed is $\frac{1}{16} \times 90^{\circ}$.
\end{enumerate}
Pereira reported that using rotations of multiples of 90 degrees performed the best, which is why I decided to implement it. I perform the rotations with a custom Keras `data generator'. Such a data generator receives as an input a batch of data and returns another batch of data. Here, the generator randomly rotates each training patch in the batch using the numpy \texttt{rot90} function, which rotates arrays by steps of 90 degrees.

\subsection{Training: Pereira's model}

\subsubsection{Architecture}

The model proposed by Pereira \cite{pereira} consists of 11 layers, shown in figure \ref{fig:pereira_model}. The first three layers are convolutional layers with filter size $3 \times 3$, stride $1 \times 1$ and width 64. Using three layers of size $3 \times 3$ consecutively effectively has a receptive field of size $7 \times 7$, but has fewer parameters than a single $7 \times 7$ convolutional layer would have, reducing the overall number of parameters and therefore making the network less prone to overfitting \cite{very_deep_conv_nets}. The next layer is a max pooling layer of size $3 \times 3$ and stride $2 \times 2$. This is an unusual design choice for convolutional networks as the size is typically smaller than the stride. Pereira motivates this choice by arguing that although pooling can be positive to eliminate unwanted details and achieve invariance, it can also eliminate some of the important details. By keeping the size of the layer larger than the stride, the pooling will overlap which will allow the network to keep more information about location. Next, three more consecutive convolutional layers are applied, this time doubling the width to 128 filters. Again, a max pooling layer with the same hyper-parameters as the previous one is applied, before adding three fully-connected layers of size 256, 256 and 5 respectively. Table \ref{table:pereira_weights} shows the model architecture in more details including the number of weights in each layer. 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{pereira_model}
	\caption{Convolutional network architecture proposed by Pereira.}
	\label{fig:pereira_model}
\end{figure}

\begin{table}
\centering	
\begin{tabular}{ c c S[table-format=7.0] } 
\textbf{Layer type} & \textbf{Output shape} & \textbf{\# Parameters} \\
 \hline
 Conv 		& $\ 64 	\times 33 	\times 33$ 	& 2368 \\ 
 Conv 		& $\ 64 	\times 33 	\times 33$ 	& 36928 \\ 
 Conv 		& $\ 64 	\times 33 	\times 33$	& 36928 \\ 
Max-Pool 	& $\ 64 	\times 16 	\times 16$ 	& 0\\
 Conv 		& $128 		 \times 16 	\times 16$	& 73856 \\ 
 Conv 		& $128 		\times 16 	\times 16$ 	& 147584 \\ 
 Conv 		& $128 		\times 16 	\times 16$ 	& 147584 \\ 
Max-Pool 	& $128 		\times\ 7 	\times\ 7$	& 0\\
FC			& $256 		\times\ 1 	\times\ 1$	& 1605888\\
FC			& $256 		\times\ 1 	\times\ 1$	& 65792\\
FC			& $\quad 5 	\times\ 1 	\times\ 1$ 	& 1285\\
\hhline{~~=}
\rule{0pt}{3ex}    
&& 2118213\\
\end{tabular}
\caption[Summary of the architecture proposed by Pereira.]{Summary of the architecture proposed by Pereira, including the number of parameters in each layer. The network has a total of 2,118,213 trainable parameters.}
\label{table:pereira_weights}
\end{table}


The weights in the convolutional layers are initiliased using Xavier normal initialisation \cite{xavier_init}. The biases are initialised to the constant value 0.1, except for the last layer.

The activation function used throughout the network is the leaky rectifier, with  $\alpha = 0.333$.

Of the three techniques presented for avoiding overfitting in the preparation chapter, Pereira uses Dropout \cite{dropout} in the fully-connected layers, with probability $p=0.1$ of removing a node from the network.

\subsubsection{Implementation of the architecture in Keras}
The network proposed by Pereira is sequential, that is the layers are stacked linearly. Using the Keras \texttt{Sequential} model is therefore the simplest way to implement it. The \texttt{Sequential} model makes it possible to create networks by adding layers to it sequentially. The three types of layers used in the network have available Keras implementations, and therefore building the network itself was relatively straightforward. First, an instance of a sequential model is instantiated:
\begin{lstlisting}[style=python, language=Python]
	model = Sequential()
\end{lstlisting}
Then, layers can be added by calling the \texttt{add} function on the model. For example, to add a convolutional layer, we first create an instance of the \texttt{Convolution2D} class and then add it to the model:
\begin{lstlisting}[style=python, language=Python]
	conv = Convolution2D(nb_filters, width, height, border_mode='same', init='glorot_normal')
	model.add(conv)
\end{lstlisting}
The library makes it easy for the user to specify how the weights should be initialised by setting the \texttt{init} argument. The \texttt{border\_mode} argument determines the size of the padding that is added to input before applying the convolutional layer. In our case the output of the layer should have the same size as the input, which is specified by setting the argument to `same'. Max pooling layers and fully-connected layers can be added similarly using instances of \texttt{MaxPooling2D} and \texttt{Dense} classes respectively.

Activation functions are added in the same way as layers. Hence, to add a leaky rectifier, also called LReLU, we add an instance of the \texttt{LeakyReLU} class to the model:
\begin{lstlisting}[style=python, language=Python]
	alpha = 0.333
	LReLU = LeakyReLU(alpha)
	model.add(LReLU)
\end{lstlisting}
Adding Dropout is done similarly:
\begin{lstlisting}[style=python, language=Python]
	p = 0.1
	model.add(Dropout(p))
\end{lstlisting}

Finally, before training the model, we need to specify which loss function and which algorithm should be used to respectively specify and minimise the loss function. In Keras this is called `compiling' the model:
\begin{lstlisting}[style=python, language=Python]
	sgd = SGD(lr=3e-5, decay=0.0, momentum=0.9, nesterov=True)
	model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']
\end{lstlisting}

The complete implementation of the model is shown in Appendix \ref{appendix:pereira_model}. 

\subsubsection{Training}
Pereira's paper specifies that the model was trained over 20 epochs, each training the network on 450,000 patches, that is 90,000 patches per class. The optimisation algorithm used is a standard stochastic gradient descent algorithm with Nesterov momentum, with constant momentum $\mu = 0.9$. The learning rate is linearly decreased from $3 \times 10^{-5}$ to $3 \times 10^{-7}$. Keras does not include an implementation for linear learning rate decay, thus I have to manually specify the learning rate at each epoch as shown in Algoritm \ref{alg:linear_decay}.

\begin{algorithm}
\caption{Model training with linear learning rate decay}
\label{alg:linear_decay}
\begin{algorithmic}[1]
\For{$i \text{ from } 0 \text{ to } \text{nb\_epochs}-1$} 
	\State $\text{learning\_rate} \gets \text{start\_rate} + i \times \dfrac{\text{end\_rate} - \text{start\_rate} }{ \text{nb\_epochs} - 1 }$
	\State train\_model(nb\_epochs = 1, learning\_rate)
\EndFor
\end{algorithmic}
\end{algorithm}

At the end of each epoch the loss function is evaluated on the validation data and the accuracy (equation \ref{eq:accuracy}) is computed. The loss and accuracy are then reported to the standard output. Figure \ref{fig:training_output} shows part of the output of training a network for a few epochs.
\begin{equation}
	\label{eq:accuracy}
		\text{accuracy} = 
	\frac{1}{m}\Big[\sum_{i=1}^m\mathbbm{1}[y^{(i)} = \argmax{k}P(y_{\text{pred}}^{(i)}=k)]\Big]
\end{equation}
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{training_output}
	\caption[Part of the output of the training script.]{Part of the output of the training script. The output is shown for epochs 9 and 10. Both the validation loss and the validation accuracy are reported, as well as how long it took to train for this epoch. Because all 450,000 patches cannot be held at once in memory, the epoch is divided into two training phases, each on 225,000 patches.}
	\label{fig:training_output}
\end{figure}

After training the model for 20 epochs, I save the model with its weights in a directory specified as an argument to the main Python script, so that I can reload the model for the segmentation of MRI scans. 

In Figure \ref{fig:pereira_validation_loss}, the validation loss and validation accuracy are plotted for the 20 epochs. Both seem to converge over this period. The validation accuracy is higher than the training accuracy because of Dropout: during validation, no neurons are dropped, meaning that the entire network is used, which is therefore able to classify the data more accurately.

\begin{figure}
	\centering
	\setlength\figureheight{10cm}
	\setlength\figurewidth{0.8\textwidth}
	\input{plots/pereira_validation_accuracy}
	\caption[Validation loss and accuracy during training for the model proposed by Pereira.]{Validation loss and accuracy during training for the model proposed by Pereira. Both seem to converge over this period.}
	\label{fig:pereira_validation_loss}
\end{figure}

\subsection{Training: My model}
In the second phase of my project, I designed a convolutional neural network to perform the segmentations. In this subsection, I justify the design choices I made and explain the modifications necessary for its implementation.

\subsubsection{Architecture}
The design of the architecture for my model is based on the following points:
\begin{enumerate}
	\item I increase the size of the input patches, almost doubling it to $64 \times 64$. Using this bigger patch, the model should be able to incorporate more contextual information from voxels further away in the slice for the classification. Figure \ref{fig:bigger_patches} shows the difference in patch size.
		\begin{figure}
			\centering
			\includegraphics[width=0.4\textwidth]{bigger_patches}
			\caption[Example patch for my model compared to a patch for the model proposed by Pereira.]{Example patch for my model compared to a patch for the model proposed by Pereira. \cite{pereira}. The patch for the Pereira model is drawn in red. The central blue square is the region classified by my model, of size $8 \times 8$.}
			\label{fig:bigger_patches}
		\end{figure}
	\item To speed up the segmentation and to prevent an explosion of the number of weights, instead of only classifying the central voxel, the network classifies the central $8 \times 8$ voxels of a patch. The output of the network is therefore no longer a $5 \times 1$ array of the probabilities for each class. Instead, the output is now a $5 \times 64$ array that outputs the class probabilities for each voxel in the central $8 \times 8$ `subpatch' of the input. 
	\item I use Batch Normalisation instead of Dropout to regularise the network, as Batch Normalisation has been shown to work better and significantly speed up the training process \cite{batch_normalization}. 
	\item I also add a L2 regularisation penalty to the loss function as a further method to decrease the amount of overfitting done by my model.
	\item The network is fully convolutional, that is, there are only convolutional layers and no fully-connected layers. This choice is justified by the following reasons:
	\begin{enumerate}
		\item Because no fully-connected layers are used, which are usually those with the highest number of parameters, a fully convolutional neural network will have fewer parameters. It will therefore be less prone to overfitting.
		\item The fully convolutional neural network is faster to train and can predict outputs much faster, due to the fact that it mainly applies convolutions, which are faster than the operation computed by a fully-connected layer. This, combined with the fact that the output is an $8 \times 8$ patch, will greatly decrease the time necessary to segment a entire scan.
	\end{enumerate}
\end{enumerate}

My model should therefore be able to predict classes much faster, using a larger context. The question is how using a larger output and a fully-convolutional network will affect the performance of the model. This will be discussed in the Evaluation chapter.

My model contains 13 layers, shown in table \ref{table:my_model}. All convolutional layers have filter size $3 \times 3$ and stride $1 \times 1$. The width of the filters is increased as the output shape decreases to allow the network to model more complex features. Max pooling layers are applied every 2--3 convolutional layers, to reduce the spatial input for the following layers. I apply convolutional layers and max pooling layers until the correct output dimension of $5 \times 8 \times 8$ is achieved. At this point, we have to flatten the input into a single two-dimensional array of size $5 \times 64$, onto which we can apply a softmax activation to obtain normalised class probability distribution for each one of the central 64 voxels. The kernel weights are initialised using a Xavier normal heuristic \cite{xavier_init}. The biases are all initialised to 0.

\begin{table}[h]
\centering	
\begin{tabular}{ c c S[table-format=7.0] } 
\textbf{Layer type} & \textbf{Output shape} & \textbf{\# Parameters} \\
 \hline
 Conv 		& $\ 64 	\times 64 	\times 64$ 	& 2368 \\ 
 Conv 		& $\ 64 	\times 64 	\times 64$ 	& 36928 \\ 
Max-Pool 	& $\ 64 	\times 32 	\times 32$ 	& 0\\
 Conv 		& $128 		\times 32 	\times 32$	& 73856 \\ 
 Conv 		& $128 		\times 32 	\times 32$ 	& 147584 \\ 
 Conv 		& $128 		\times 32 	\times 32$ 	& 147584 \\ 
Max-Pool 	& $128 		\times 16 	\times 16$	& 0\\
 Conv 		& $256 		\times 16 	\times 16$ 	& 295168 \\ 
 Conv 		& $256 		\times 16 	\times 16$ 	& 590080 \\ 
Max-Pool 	& $256 		\times\ 8 	\times\ 8$	& 0\\
 Conv 		& $256 		\times\ 8 	\times\ 8$	& 590080 \\ 
 Conv 		& $256 		\times\ 8 	\times\ 8$ 	& 590080 \\ 
 Conv 		& $5 		\times\ 8 	\times\ 8$ 	& 11525 \\ 
\hhline{~~=}
\rule{0pt}{3ex}    
&& 2485253\\
\end{tabular}
\caption[Summary of my fully-convolutional neural network architecture.]{Summary of my fully-convolutional neural network architecture, including the number of parameters in each layer. The network has a total of 2,485,253 trainable parameters. Although increasing the size of the input patch by a factor of 4, there are only 17\% more parameters than in the model proposed by Pereira.}
\label{table:my_model}
\end{table}

\subsubsection{Implementation}
The implementation of my model is similar to my implementation of the model proposed by Pereira, which is the main advantage of using a library such as Keras. However, there is a subtlety that arises from the fact that the network is fully-convolutional. The output of the last layer is a three-dimensional array of size $5 \times 8 \times 8$ but the softmax activation can only take as an argument two-dimensional arrays. Therefore, I have to reshape the array into a $64 \times 5$ array before applying the softmax activation. Keras makes this operation easy with the \texttt{Reshape} and \texttt{Permute} layers:
\begin{lstlisting}[style=python, language=Python]
	model.add(Reshape((5, 64)))
	model.add(Permute((2,1)))
	model.add(Activation('softmax'))
\end{lstlisting}

Similarly, using Batch Normalisation is easy in Keras, as it has a \texttt{BatchNormalization} layer built into its library. A typical convolutional layer with Batch Normalisation is implemented as follows:
\begin{lstlisting}[style=python, language=Python]
	model.add(Convolution2D(256, 3, 3, border_mode='same', init=init, W_regularizer=l2(l)))
	model.add(BatchNormalization(axis=axis, trainable=trainable))
	model.add(LeakyReLU(alpha))
\end{lstlisting}

The full implementation is listed in Appendix \ref{appendix:my_model}.

\subsubsection{Training}
I train my model for 40 epochs using 100,000 patches per epoch. To minimise the loss function I use the Adaptive Moment Estimation method (adam) \cite{adam} which is a stochastic gradient descent method that computes adaptive learning rates for each parameter. This method is implemented in Keras and can therefore be specified when compiling the model as follows:
\begin{lstlisting}[style=python, language=Python]
	adam = Adam()
    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
\end{lstlisting}

\section{Segmenting MRI scans}
\subsection{Scan normalisation}
The first stage, normalising the scans, is identical to the first stage for training the model, as explained in Section \ref{section:scan_normalisations}, and is independent of the model used to segment the scans.

\subsection{Patch extraction and segmentation}
The patch extraction and segmentation phases are, however, dependent on the model, as the patches have to be extracted such that every voxel in the input scan can be classified by the model. The implementation of these stages is more complicated for my model, and therefore I describe the implementation for the two models independently in the next two subsections.

\subsubsection{Patch extraction and segmentation for Pereira's model}
For the model proposed by Pereira, the convolutional neural network can be modelled as a function classifying the central voxel of a patch of size $33 \times 33 \times 4$. In this case, the segmentation of a patient is equivalent to convolving the function computed by the convolutional neural network on two-dimensional axial slices of the 4 input image modalities. The images must however be zero-padded around the x and y edges so as to keep the dimensions of the segmentation the same as the dimensions of the input images. Hence the steps are:
\begin{enumerate}
	\item Read in the 4 scan images as a single 4-dimensional array of size $z \times y \times x \times 4$.
	\item Normalise the image as explained in Section \ref{section:scan_normalisations}. Thus, the image is first winsorised, then the N4ITK correction is applied to it, and finally for each modality the mean and variance are normalised.
	\item Pad the x and y dimensions with zeros. Since we want the convolution operation to return an image of identical size, we need to pad with exactly half of the width of the filter, that is 16 voxels. The array has now size $z \times (y + 32) \times (x + 32) \times 4$.
	\item For each slice $z = z_i$ in the axial plane, extract all patches and store them in a list. Note that this has to be done for each slice sequentially as it would not be possible to store all patches for the entire image simultaneously in memory. For each slice there will be $y \cdot x$ patches of size $33 \times 33 \times 4$.
	\item Evaluate the convolutional neural network on each patch. Evaluating these patches in batches using a GPU makes this step significantly faster. The convolutional neural network will return a probability distribution over the 5 classes for each patch. The most likely class is chosen as the class for that patch. Record the classes in a second list.
	\item Transform the list of classes back into a two-dimensional array of size $y \cdot x$, which is the $z = z_i$ slice in the axial plane of the segmentation.
\end{enumerate}

Figure \ref{fig:example_pereira_segmentation} shows the resulting segmentation for the first challenge patient.

\begin{figure}
	\centering
	\includegraphics[scale = 0.1]{challenge_1_segmentation_66}
	\includegraphics[scale = 0.1]{challenge_1_segmentation_with_T2_66}
	\\
	\vspace{0.5cm}
	\includegraphics[scale = 0.1]{challenge_1_segmentation_71}
	\includegraphics[scale = 0.1]{challenge_1_segmentation_with_T2_71}
	\\
	\vspace{0.5cm}
	\includegraphics[scale = 0.1]{challenge_1_segmentation_76}
	\includegraphics[scale = 0.1]{challenge_1_segmentation_with_T2_76}
	
	\caption[Example segmentation computed using the model proposed by Pereira.]{Example segmentation computed using the model proposed by Pereira. Left column: The segmentation for patient 1 in the challenge dataset for axial plane slices for $z \in \{66, 71, 76\}$. Right column: The same segementation slices overlaid on the T2 scan.}
	\label{fig:example_pereira_segmentation}
\end{figure}

\subsubsection{Patch extraction and segmentation for my model}
As my model classifies the central $8 \times 8$ voxels for each input patch, the segmentation is slightly more complicated. The padding has to be done differently, as it depends on the size of the image. This is because we are effectively convolving the model on the slices with a stride of $8 \times 8$ and thus the last application of the model in a row or column might not fit if the width or height of the slice is not a multiple of 8. To fix this, I pad the end of the x and y axes by an extra 8 voxels. Thus, the model is padded by $\frac{64 - 8}{2} = 28$ zeros in front of the x and y axis, and the end is padded with $\frac{64 -8}{2} + 8 = 36$ zeros. Then, for each slice in the image, the steps computed are:
\begin{enumerate}
	\item Extract all patches of size $64 \times 64 \times 4$ for the entire slice, so that the center $8 \times 8$ squares of each patch do not overlap but cover the entire input image. This can be done after padding by two nested loops with step increments of 8.
	\item Iterate over the patches in batches, such that each batch fits into the memory of the GPU and use the convolutional neural network to predict the class for the central $8 \cdot 8 = 64$ voxels of each patch. These are returned in a one-dimensional array.
	\item Concatenate all the predictions for all batches into a single one-dimensional array of length $64 \cdot \#patches$. This one-dimensional array has to be transformed back into a two-dimensional array such that each voxel is mapped back into its original position in the input image.
	\item Calculate the number of patches that fit into the slice as follows:
		\begin{equation}
		\begin{split}
			\text{height}_p & \gets \bigg \lceil \cfrac{\text{image\_height}}{8} \bigg \rceil 	 \\
			\text{width}_p & \gets \bigg \lceil \cfrac{\text{image\_width}}{8} \bigg \rceil 	 \\
		\end{split}		
		\end{equation}
	\item Reshape the voxels into a single 4-dimensional array of size $(\text{height}_p \times \text{width}_p \times 8 \times 8)$
	\item Concatenate the array along its first axis. This can be done using the \texttt{concatenate} function provided by numpy. The result is a 3-dimensional array of size $(\text{height}_p \times 8 \cdot \text{width}_p \times 8)$. This will concatenate individual patches along rows together.
	\item Similarly, concatenate the new array along its first axis. This will concatenate the columns, resulting in a single 2-dimensional array of size $(8 \cdot \text{height}_p \times 8 \cdot \text{width}_p)$.
	\item Finally, remove the extra voxels that might have been added by the extra padding. This can be done using numpy to trim the array to the image width and height. The result will be a 2-dimensional array which is the segmented slice.
\end{enumerate}


\subsection{Segmentation post-processing}
After the new patient has been segmented, a further simple heuristic is used to remove small volumes of data labelled as diseased tissue, based on the assumption that the diseased tissue is connected into a single component of relatively large size. Pereira \cite{pereira} proposed to remove all connected components (continuous regions of diseased tissue) of less than 10,000 voxels in volumes.

I implement this heuristic using a depth first search on the graph represented by a scan, where each voxel is a vertex and shares an edge with its 6 immediate neighbours (or less if it is on an edge). First, I create a second 3-dimensional array of the same size as the scan. This array marks each node as unseen, visited or processed. As only the diseased tissue is considered, i.e.\ classes 1--4, I mark all voxels of class 0 as already processed. Then the algorithm iterates the following steps until all voxels have been processed:
\begin{enumerate}
	\item Initialise an empty list, to represent the next connected component $C$.
	\item Take a voxel that has not been visited, mark it as processed and add its coordinates to $C$.
	\item Initialise a queue containing all neighbours of the voxel that have not yet been visited or processed and are segmented as diseased tissue. Mark those neighbours as visited.
	\item Then, repeat while the queue is not empty:
	\begin{enumerate}
		\item Pop the first element, mark it as processed, and add its coordinates to the connected component $C$.
		\item Find its neighbours that have not been visited or processed. Mark those as visited and push those to the end of the queue. Repeat step 4.
	\end{enumerate}
	\item Once the queue is empty, add the connected component $C$ to a list containing all disjoint connected components. Repeat step 1 until all nodes have been processed.
\end{enumerate}
Finally, I iterate over the list of connected components, finding those which have less voxels than the threshold size and setting the segmentation of those voxels to 0. Figure \ref{fig:connected_components_example} shows the effect of applying this post-processing step to the last challenge scan.
\begin{figure}[h]
	\centering
	\includegraphics[scale = 0.1]{challenge10_no_post_77}
	\includegraphics[scale = 0.1]{challenge10_with_post_77}
	\\
	\vspace{0.5cm}
	\includegraphics[scale = 0.1]{challenge10_no_post_87}
	\includegraphics[scale = 0.1]{challenge10_with_post_87}
	\caption[Effect of applying the post-processing step.]{Effect of applying the post-processing step to the 10\textsuperscript{th} challenge scan. Axial plane slices $z \in \{77,87\}$ are shown side-by-side (before the post-processing step on the left, and after the post-processing step on the right). Small connected components of diseased tissue of volume below a threshold are removed.} 
	\label{fig:connected_components_example}
\end{figure}

\chapter{Evaluation}

\section{Unit tests}
To verify that the stages behave as expected, I implement some unit tests using the Python \texttt{unittest}\footnote{\url{https://docs.python.org/3/library/unittest.html}} module as it does not require any additional libraries. Furthermore, the module was inspired by JUnit\footnote{\url{http://junit.org/junit4/}}, which I had previously used during other projects.

 For the normalisation stage, I used the ANTs neuroimaging tools\footnote{\url{http://stnava.github.io/ANTs/}} and \texttt{scipy}\footnote{\url{https://www.scipy.org/}} to perform the N4 correction and the winsorising step respectively, thus I only tested the mean and standard deviation normalisation. I tested the patch extraction phase with small 4-dimensional arrays, to check that the correct patches are returned and that the patch labels are balanced. The data augmentation stage does not need to be unit tested as it is implemented with a Keras `data generator'. I tested the segmentation stage similarly, using small arrays and a simple classifying function to mock the behaviour of the convolutional neural network. Finally, I also tested the post-processing stage for different inputs with known outputs.
 
As an example, I include two of the tests for the post-processing step:
\begin{lstlisting}[style=python, language=Python]
	class TestRemoveConnectedComponents(unittest.TestCase):

    def test_no_components(self):
        image = np.full(shape=(3,3,3), fill_value=0)
        image2 = np.copy(image)
        remove_connected_components(image2, 1, verbose=False)
        self.assertTrue(np.array_equal(image, image2))
    
    def test_remove_one_component(self):
        image = np.array([[[0,0,0],[0,0,0],[0,0,0]],
                          [[0,0,0],[0,1,0],[0,0,0]],
                          [[0,0,0],[0,0,0],[0,0,0]]])
        remove_connected_components(image, 2, verbose=False)
        self.assertTrue(np.array_equal(image, np.full(image.shape, 0)))
\end{lstlisting}

Figure \ref{fig:unit_test_output} shows the output of running the unit tests. All unit tests pass, indicating that the tested stages behave as expected.

\begin{figure}[h]
	\centering
	\includegraphics[width = \textwidth]{unit_test_output}
	\caption{Output of running the unit tests. All tests pass successfully.}
	\label{fig:unit_test_output}
\end{figure}

\section{BraTS evaluation}
\subsection{Evaluation metrics}
The BraTS2013 challenge \cite{brats-proceedings} evaluates the segmentation using three different metrics: \textbf{positive predictive value} (or precision), \textbf{sensitivity} (or recall) and \textbf{dice score}. These metrics are best visualised using the Venn diagram shown in figure \ref{fig:evaluation_venn_diagram}.  Note that accuracy is not used as a metric. This is because the data is highly unbalanced, as shown previously, meaning that a model trivially labelling everything with class 0 would reach an accuracy of 99\% or more, making it very hard to compare different models.

\begin{figure}
	\centering
	\includegraphics[scale = 0.5]{evaluation_venn_diagram}
	\caption{Venn diagram of the classification space.}
	\label{fig:evaluation_venn_diagram}
\end{figure}

\subsubsection{Positive predictive value}
The positive predictive value is defined as
\begin{equation}
	\textrm{PPV} = \frac{\textrm{TP}}{\textrm{TP} + \textrm{FP}}
\end{equation}
where \textrm{TP} and \textrm{FP} are the numbers of true and false positives respectively. In the Venn diagram in figure \ref{fig:evaluation_venn_diagram}, this corresponds to the size of the green and blue area relative to the size of the entire blue area.  Since the denominator $\textrm{TP} + \textrm{FP}$ is the total number of positively classified elements, the positive predictive value gives an indication as to how accurately the model can predict a positive class. In the case of brain tumour segmentations, the positive predictive value measures how confidently the model is predicting tumours. Thus, a high positive predictive value means those regions labelled as tumours by the model really are tumours.

\subsubsection{Sensitivity}
The sensitivity is defined as 
\begin{equation}
	\textrm{Sensitivity} = \frac{\textrm{TP}}{\textrm{TP} + \textrm{FN}}
\end{equation}
where \textrm{FN} is the number of false negative classifications and \textrm{TP} is the number of true positives. The sensitivity measures the size of the blue and green region relative to the size of the entire green region. As the denominator $\textrm{TP} + \textrm{FN}$ equals the number of positively labelled elements in the ground truth, the sensitivity measures how well a model is able to recognise positively labelled examples. In the case of brain tumour segmentations, the sensitivity measures how good a model is at recognising a tumour. A high sensitivity would indicate that the model is able to recognise a high fraction of the positively labelled voxels.

\subsubsection{Trade-off between positive predictive value and sensitivity }
There is a trade-off between the sensitivity and the positive predictive value. A model classifying everything as negative would reach perfect positive predictive value as there would be no false positives, but would have a sensitivity of 0. Similarly, a model classifying everything as positive would reach perfect sensitivity as there would be no false negatives, but would have a positive predictive value of 0. For brain tumours, a high sensitivity is arguably more important because missing a tumour can have fatal consequences. However, this has a trade-off with the utility of a model, since a low positive predictive value also means that we can only have little confidence in the positive predictions made by it. Thus, the Dice score is used to mitigate this trade-off, as explained next.

\subsubsection{Dice score}
The Dice score is defined in terms of both the false negatives and the false positives, and thus defines a metric which tries to take this trade-off into account. The Dice score is defined as
\begin{equation}
\begin{split}
	\textrm{Dice Score} & = \cfrac{\vert \textrm{Ground truth} \cap \textrm{Prediction}\vert}{\frac{1}{2} \cdot (\vert \textrm{Ground truth}\vert + \vert \textrm{Prediction}\vert)} \\
	&  = \cfrac{2 \cdot \textrm{TP}}{\textrm{FP} + 2 \cdot \textrm{TP} + \textrm{FN}}
\end{split}
\end{equation}
and therefore calculates the area corresponding to the true positives relative to the average size of the two segmented areas.

Figure \ref{fig:evaluation_example} shows an example for a segmented slice in the axial plane.  The metrics for this segmentation are
\begin{equation*}
\begin{split}
	\textrm{PPV} & = \cfrac{\vert P_1 \cup T_1 \vert}{\vert P_1 \vert} \\
	\textrm{Sensitivity} & = \cfrac{\vert P_1 \cup T_1 \vert}{\vert T_1 \vert} \\
	\textrm{Dice score} & = \cfrac{\vert P_1 \cup T_1 \vert}{\frac{1}{2} \cdot (\vert P_1 \vert + \vert T_1 \vert)} \\
\end{split}
\end{equation*}

\begin{figure}[h]
	\centering
	\includegraphics[scale = 0.5]{example_evaluation}
	\caption[Example showing the segmentation space for a slice in the axial plane.]{Example showing the segmentation space for a slice in the axial plane. This image was reproduced from \cite{brats-proceedings}.}	
	\label{fig:evaluation_example}
\end{figure}

\subsection{Regions of evaluation}
As stated above, the metrics are defined only for binary classification into positive and negative classes. However, our segmentations are classified into 5 different classes (0--4). Therefore, different `regions' define which classes are counted as positive and which ones as negative. For the BraTS challenge there are three regions:
\begin{enumerate}
	\item \textbf{Complete}: All classes 1--4 are defined as positive, only 0 as negative. The metrics for the complete scores therefore evaluate how well a model is able to discern diseased tissues from normal tissues.
	\item \textbf{Core}: Classes 1,3 and 4 are defined as positive, 0 and 2 as negative. In this region, the edema class is no longer considered as positive.
	\item \textbf{Enhancing}: Only class 4, the enhacing tumour class is defined as positive. The metrics for this region measure how well the model can segment the enhancing tumour class only.
\end{enumerate}


\section{Evaluation of the model proposed by Pereira}
\subsection{BraTS evaluation}
\subsubsection{Dice score}
Table \ref{table:pereira_dice_results} shows the Dice scores I obtained for each of the 10 challenge scans using my implementation of the method proposed by Pereira \cite{pereira}. In figure \ref{pereira_box_plot} the mean and standard deviation are plotted for the three regions, along with the Dice score reported by Pereira in his paper. 
\begin{table}
\centering	
\begin{tabular}{ c | c c c} 
\multirow{2}{*}{\textbf{Patient}} & \multicolumn{3}{c}{\textbf{Dice score}} \\
 & Complete & Core & Enhancing \\
 \hline
1 & 0.813 & 0.843 & 0.472 \\
2 & 0.796 & 0.700 & 0.619 \\
3 & 0.808 & 0.765 & 0.345 \\
4 & 0.723 & 0.426 & 0.178 \\
5 & 0.750 & 0.676 & 0.550 \\
6 & 0.776 & 0.510 & 0.518 \\
7 & 0.818 & 0.035 & 0.034 \\
8 & 0.829 & 0.878 & 0.700 \\
9 & 0.881 & 0.883 & 0.818 \\
10 & 0.852 & 0.840 & 0.840 \\
\hline
\rule{0pt}{3ex}    
\textbf{mean} & $0.805 \pm 0.047$ & $0.656 \pm 0.267$ & $0.508 \pm 0.262$
\end{tabular}
\caption[Dice scores obtained for the 10 challenge patients using my implementation of the method proposed by Pereira.]{Dice scores obtained for the 10 challenge patients using my implementation of the method proposed by Pereira. These results were produced by the online evaluation platform as the ground truth labelling is not publicly available.}
\label{table:pereira_dice_results}
\end{table}

\begin{figure}
	\centering
%	\setlength\figureheight{10cm}
%	\setlength\figurewidth{0.8\textwidth}
%	\input{plots/pereira_box_plot}
	\includegraphics[width=0.8\textwidth]{plots/pereira_box_plot2}
	\caption{Box plot showing the Dice scores obtained by my implementation of the model proposed by Pereira.}
	\label{pereira_box_plot}
\end{figure}

The mean scores for the `Core' and `Enhancing' region are significantly lower than the mean score for the `Complete' region, and have a much higher standard deviation. This can partially be explained by the outlier values obtained from patient 7. For this patient, the diseased tissue has been segmented correctly, but the model was not able to distinguish the different classes within the diseased tissue, and it classified almost everything as either normal tissue (class 0) or edema (class 2). Because class 2 is not defined as positive in the `Core' and `Enhancing' regions, the scores for those regions are very close to 0.  

However, even if the results from patient 7 were ignored, the average results for regions 2 and 3 are still below those reported by Pereira \cite{pereira} of (0.8, 0.78, 0.73). As I exactly followed the method described in the paper published by Pereira, I emailed the author asking for a possible explanation for this difference. He answered that (at the time) there were two differences between his and my method, which he did not report in his paper. The first difference was the parameters used in the N4 correction, which he did not include in his paper. However, changing the parameters I use to those used in his proposed model did not have an impact on the results. The second difference was how the patches are selected. The author of the paper wrote in his email, shown in Appendix \ref{appendix:pereira_email}, that he used a heuristic to avoid selecting patches that were too close to each other (less than 3 voxels apart) and made sure to include patches of normal tissue close to the tumour. I also implemented these heuristics as described in Section \ref{section:patch_extraction}, which improved the results slightly but not to a level similar to those reported by the author. This discrepancy is hard to explain, but might be due to the fact that the heuristics I implemented are only approximations to those used (but not reported) by Pereira.

\subsubsection{Sensitivity and positive predictive value}
The means and standard deviations for the sensitivity and positive predictive value for my implementation of the model proposed by Pereira are reported in Table \ref{table:pereira_sensitivity_average}. My implementation obtained higher values for the positive predictive value but lower scores for the sensitivity. This can be explained by the trade-off that exists between these two metrics, mentioned previously.

\begin{table}
\centering	
{\def\arraystretch{1.3}\tabcolsep=5pt
\begin{tabular}{ c  c | c | c | c} 
& \multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Region}} \\
& & Complete & Core & Enhancing \\
\cline{2-5}
\multirow{2}{*}{\textbf{Sensitivity}} & My implementation &	 $0.816 \pm 0.135$ & $0.588 \pm 0.304$  & $0.429 \pm 0.289$ \\
& Reported by Pereira  & 0.92 & 0.74 & 0.77 \\
\cline{2-5}
\multirow{2}{*}{\textbf{PPV}} & My implementation & $0.820 \pm 0.092$ & $0.910 \pm 0.074$ & $0.861 \pm 0.096$ \\
& Reported by Pereira & 0.75 & 0.86 & 0.71 \\
\end{tabular}
}
\caption[Sensitivity and positive predictive value for the three regions obtained on the challenge dataset using my implementation of the method proposed by Pereira.]{Sensitivity and positive predictive value for the three regions obtained on the challenge dataset using my implementation of the method proposed by Pereira. The results reported by Pereira are also shown for comparison. My implementation achieves higher values for the positive predictive value but lower values for the sensitivity.}
\label{table:pereira_sensitivity_average}
\end{table}

\subsubsection{BraTS ranking}
I submitted my segmentations to the online evaluation platform which ranks the different contestants according to their scores. Figure \ref{fig:online_eval_rank} shows a screenshot of part of the ranking table that shows the entry for my submission. My submission ranked 2\textsuperscript{nd} and 1\textsuperscript{st} for the positive predictive value on the core and enhancing regions respectively, but ranked low for the sensitivity on those regions. This suggests that my model finds tumour with very high reliability, but doesn't recognise all tumours and shows that there is indeed a trade-off between positive predictive value and sensitivity.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{ranking_table_header}
	\includegraphics[width=\textwidth]{pereira_model_ranked_results}
	\caption[Online evaluation ranking.]{Online evaluation ranking. My submission was ranked 42\textsuperscript{nd} out of 55 contestants (at the time of writing).}
	\label{fig:online_eval_rank}
\end{figure}

\subsection{Confusion matrix}
To get more insights into how the model behaves, I decided to segment 11 scans from the 2015 dataset and compute the confusion matrix. The Dice scores obtained for these 11 scans are 0.681, 0.643 and 0.673 for the `Complete', `Core' and `Enhancing' regions respectively. The ground truth for these scans is put together slightly differently than for the 2013 dataset, which explains why the Dice score is lower for the `Complete' region. However, the scans are still similar enough for these scores and the confusion matrix to be relevant. The confusion matrix is shown in table \ref{table:confusion_pereira}. As expected from the Dice scores, the model is doing well for classes 0 and 4, predicting over 64\% of the voxels in class 4 correctly. For classes 1, 2 and 3 the model is doing rather poorly, reaching no more than 38\% of correctly classified voxels and predicting a majority of voxels as being from class 0. The especially low result for class 3, with only about 11\% of correctly classified voxels can be explained by the fact that there is less training data available for this class. Furthermore, because the boundaries between classes 2, 3 and 4 are somewhat subjective, there are different possible interpretations of the tissues.

\begin{table}[h]
\centering	
\setlength{\tabcolsep}{10pt}
\begin{tabular}{c c S[table-format=2.3] S[table-format=2.3] S[table-format=2.3] S[table-format=2.3] S[table-format=2.3]} 
& & \multicolumn{5}{c}{\textbf{Predicted}} \\
\rule{0pt}{3ex}& & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} \\
\cline{3-7}
\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Ground truth}}} & \multicolumn{1}{c|}{0} & \textbf{99.94} & 0.001 & 0.052& 0.002 & 0.001 \\
& \multicolumn{1}{c|}{1} & \textbf{42.287} & 25.361 & 5.059 & 17.710 & 9.582 \\
& \multicolumn{1}{c|}{2} & \textbf{55.743} & 1.508 & 38.762 & 3.065 & 0.921 \\
& \multicolumn{1}{c|}{3} & \textbf{51.163} & 8.576 & 23.191 & 10.702 & 6.367 \\
& \multicolumn{1}{c|}{4} & 25.067 & 0.891 & 3.926 & 6.107 & \textbf{64.0078} \\
\end{tabular}
\caption[Confusion matrix obtained with my implementation of the model proposed by Pereira on 11 scans taken from the BraTS 2015 dataset.]{Confusion matrix obtained with my implementation of the model proposed by Pereira on 11 scans taken from the BraTS 2015 dataset. The percentage of correctly predicted voxels for each class is shown.}
\label{table:confusion_pereira}
\end{table}

\subsection{Effect of various components}
Finally, I investigate what impact different components and design choices for the convolutional neural networks have on the final Dice score on the challenge dataset. I investigate the following design choices:
\begin{enumerate}
	\item Using the linear rectifier (ReLU) activation function \ref{eq:linear_rectifier} instead of the Leaky linear rectifier.
	\item Using the Adaptive Moment Estimation method (adam) \cite{adam} to update the learning rates automatically, instead of linearly decaying the learning rate.
\end{enumerate}
In table \ref{table:variants_dice_results} the results obtained using these variants are shown. The variant using the adam optimisation performs better than my implementation of Pereira's model which uses linearly decaying learning rates. This suggests that my implementation of the model proposed by Pereira might not have fully converged after 20 epochs and would need further training and learning rate adaptation to reach the same convergence as the model using the adam optimisation.

\begin{table}
\centering	
\begin{tabular}{ c | c c c} 
\multirow{2}{*}{\textbf{Variant}} & \multicolumn{3}{c}{\textbf{Dice score}} \\
 & Complete & Core & Enhancing \\
 \hline
Adam + ReLU & $0.796 \pm 0.054$ & $0.630 \pm 0.279$ & $0.486 \pm 0.265$ \\
Adam optimiser & $0.815 \pm 0.069$ & $0.670 \pm 0.233$ & $0.592 \pm 0.217$ \\
\end{tabular}
\caption{Dice scores obtained using variants of the model proposed by Pereira.}
\label{table:variants_dice_results}
\end{table}


\section{Evaluation of my model}
\subsection{BraTS evaluation}
Tables \ref{table:my_model_dice_results} and \ref{table:my_model_sensitivity_ppv} show the results I obtained using my fully-convolutional model on the 10 images included in the challenge dataset. The Dice scores for patient number 7 are again considerably lower than those obtained on the other patients in the `Core' and `Enhancing' regions. This is hard to explain, as Pereira reported no such discrepancy. A possible explanation is that the images for patient 7 are slightly rotated in the x-y plane, as shown in Figure \ref{fig:patient_7_t2}.

\begin{table}
\centering	
\begin{tabular}{ c | c c c} 
\multirow{2}{*}{\textbf{Patient}} & \multicolumn{3}{c}{\textbf{Dice score}} \\
 & Complete & Core & Enhancing \\
 \hline
1 & 0.856 & 0.796 & 0.665 \\
2 & 0.843 & 0.622 & 0.670 \\
3 & 0.744 & 0.297 & 0.368 \\
4 & 0.379 & 0.156 & 0.150 \\
5 & 0.812 & 0.689 & 0.665 \\
6 & 0.772 & 0.381 & 0.493 \\
7 & 0.593 & 0.015 & 0.015 \\
8 & 0.877 & 0.705 & 0.675 \\
9 & 0.725 & 0.857 & 0.781 \\
10 & 0.898 & 0.905 & 0.838 \\
\textbf{mean} & $0.750 \pm 0.158$ & $0.542 \pm 0.310$ & $0.532 \pm 0.273$
\end{tabular}
\caption{Dice scores obtained for the 10 challenge patients using my model.}
\label{table:my_model_dice_results}
\end{table}

\begin{table}
\centering	
\begin{tabular}{ c | c | c} 
\textbf{Region} & \textbf{Mean sensitivity } & \textbf{Mean positive predictive value} \\
\hline
Complete &	$0.661 \pm 0.209$ & $0.933 \pm 0.042$ \\
Core & 		$0.453 \pm 0.318$ & $0.945 \pm 0.038$ \\
Enhancing & $0.467 \pm 0.297$ & $0.840 \pm 0.101$ \\
\end{tabular}
\caption{Sensitivity and positive predictive value obtained on the challenge dataset using my model.}
\label{table:my_model_sensitivity_ppv}
\end{table}

\begin{figure}[h]
	\centering
	\includegraphics[scale = 0.3]{patient7_t2}
	\caption[Slice of the T2 scan for patient 7 of the challenge dataset.]{Slice of the T2 scan for patient 7 of the challenge dataset. The image is slightly rotated or skewed in the x-y plane, which is a possible explanation for the lower results in the `Core' and `Enhancing' regions for that patient.}
	\label{fig:patient_7_t2}
\end{figure}

\subsection{Confusion matrix}
The confusion matrix obtained for my model using the same 11 patients of the BraTS2015 dataset is shown in Table \ref{table:confusion_my_model}. Interstingly, although my model performs worse on the 2013 challenge dataset than the model proposed by Pereira, it performs much better on this subset of the 2015 training dataset, obtaining average Dice scores of (0.806,0.776,0.819). The lower value for class 3 can again be explained by the fact that there is less training data available for this class. However, compared to my implementation of Pereira's model the value has more than tripled from 11\% to 38\% of voxels from class 3 being correctly classified.

\begin{table}
\centering	
\setlength{\tabcolsep}{10pt}
\begin{tabular}{c c S[table-format=2.2] S[table-format=2.2] S[table-format=2.2] S[table-format=2.2] S[table-format=2.2]} 
& & \multicolumn{5}{c}{\textbf{Predicted}} \\
\rule{0pt}{3ex}& & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} \\
\cline{3-7}
\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Ground truth}}} & \multicolumn{1}{c|}{0} & \textbf{99.78} & 0.02 & 0.16 & 0.02 & 0.02 \\
& \multicolumn{1}{c|}{1} & 0.74 & \textbf{42.66} & 20.58 & 26.91 & 9.11 \\
& \multicolumn{1}{c|}{2} & 28.79 & 4.13 & \textbf{57.97} & 7.54 & 1.57 \\
& \multicolumn{1}{c|}{3} & 5.87 & 10.49 & 30.81 & \textbf{37.98} & 14.84 \\
& \multicolumn{1}{c|}{4} & 4.44 & 1.62 & 4.77 & 8.52 & \textbf{80.64} \\
\end{tabular}
\caption[Confusion matrix obtained with my model on 11 scans taken from the BraTS 2015 dataset.]{Confusion matrix obtained with my model on 11 scans taken from the BraTS 2015 dataset. The percentage of correctly predicted voxels for each class is shown.}
\label{table:confusion_my_model}
\end{table}

\section{Comparison}
\subsection{Performance}
Table \ref{table:comparison} shows the results I obtained on the BraTS2013 Challenge dataset along to those reported in Pereira's paper. My implementation of the model proposed by Pereira with adam optimisation has the highest Dice score for the `Complete' region and therefore improves on the results reported by Pereira for this particular region. However, the Dice scores for the `Core' and the `Enhancing' regions are lower than those reported by Pereira, as explained previously. My fully-convolutional architecture has lower Dice scores but obtains the highest positive predictive value on all 3 regions. This suggests that this model could further be improved by attempting to trade-off some positive predictive value for sensitivity to obtain higher Dice scores. 

\begin{table}[h]
\centering	
\resizebox{\textwidth}{!}{\begin{tabular}{c | c c c | c c c | c c c}
\multirow{2}{*}{Method} & \multicolumn{3}{c}{\underline{Dice}} & \multicolumn{3}{c}{\underline{PPV}} & \multicolumn{3}{c}{\underline{Sensitivity}} \\
 & Complete & Core & Enhancing & Complete & Core & Enhancing & Complete & Core & Enhancing  \\
 \hline
  My FCN model &  0.75 & 0.54 & 0.53 & \textbf{0.93} & \textbf{0.95} & \textbf{0.84} & 0.66 & 0.45 & 0.47\\
 My Pereira impl.\ & 0.81 & 0.66 & 0.51 & 0.82 & 0.91 & 0.86 & 0.82 & 0.59 & 0.43\\
 Variant with Adam & \textbf{0.82} & 0.67 & 0.53 & 0.89 & 0.87 & 0.80 & 0.78 & 0.61 & 0.54 \\
 Reported by Pereira & 0.80 & \textbf{0.78} & \textbf{0.73} & 0.75 & 0.86 & 0.71  & \textbf{0.92} & \textbf{0.74} & \textbf{0.77} \\
\end{tabular}}
\caption{Comparison of model performances on the Brats2013 Challenge dataset.}
\label{table:comparison}
\end{table}

The average Dice score over the 11 evaluated scans of the 2015 dataset is shown in Table \ref{table:2015_dice}. The fully-convolutional model performed much better than my implementation of the model proposed by Pereira. This can possibly be explained by the fact that the 2015 dataset is normalised differently and that the fully-convolutional model has a much larger receptive field.
\begin{table}
\centering	
\begin{tabular}{c | c c c }
\multirow{2}{*}{Method} & \multicolumn{3}{c}{\underline{Dice}} \\
 & Complete & Core & Enhancing \\
 \hline
 My fully-convolutional model & 0.806 & 0.776 & 0.819\\
 My implementation of Pereira's model.\ & 0.681 & 0.643 & 0.673\\
\end{tabular}
\caption{Dice scores obtained on the 2015 dataset for my FCN model and my implementation of the model proposed by Pereira.}
\label{table:2015_dice}
\end{table}

\subsection{Segmentation speed}
Finally, I compared how fast the two implementations can segment a scan of $155 \times 240 \times 240$ voxels by taking the average time needed to segment 11 scans of the 2015 dataset. As expected, the fully-convolutional model is much faster, achieving a speedup of a factor of 7.25 , as shown in table \ref{table:time_comparison}.
\begin{table}[h]
\centering	
\begin{tabular}{c | c }
Method & Time to segment a scan (in seconds)\\
 \hline
 My FCN model & 110.47 \\
 My Pereira impl.\ & 802.82 \\
\end{tabular}
\caption[Comparison of the time required for segmenting a scan of $155 \times 240 \times 240$ voxels.]{Comparison of the time required for segmenting a scan of $155 \times 240 \times 240$ voxels for my implementation of the model proposed by Pereira and my fully-convolutional model.}
\label{table:time_comparison}
\end{table}

\chapter{Conclusion}
\section{Summary of achievements}
This project surveyed the mathematical foundations of deep learning with convolutional neural networks and how these models can be applied to the problem of brain tumour segmentation. I implemented two different convolutional neural networks. First, I replicated the model proposed by Pereira et al.\ \cite{pereira}. This provided a good starting point in this field with some certainty that the model would work. The model I implemented performed better in the `Complete' region than Pereira's model, but performed worse in the `Core' and `Enhancing' region, obtaining respective average Dice scores of (0.82, 0.67, 0.53) for the BraTS2013 \cite{brats-proceedings} challenge. I then designed a second model using techniques more recently developed for the design of convolutional neural networks. The model I built is performing similarly, but has the significant advantage of being able to segment a scan up to 7 times faster. This speed-up is achieved by removing all fully-connected layers from the network. This also has the effect of reducing the number of parameters, which makes my convolutional neural networks able to consider an input patch with 4 times more voxels, while keeping the number of parameters similar. Both models were trained on the data made available through the BraTS2013 challenge.

I evaluated both models using the standard metrics for this segmentation task: Dice score, positive predictive value and sensitivity. I also compared the performance of my models to the performance of models published by researchers in the field using the BraTS challenge leaderboard. Using the data from the BraTS 2015 dataset, I was further able to get more insight in how well the different models perform by examining the confusion matrix for this dataset.

Both models approximated the performance obtained by recent state-of-the-art methods for the `Complete' region, but not for the `Core' or `Enhancing' regions. Especially noticeable is the difference in the Dice score for the `Enhancing' tumour region between my implementation of the model proposed  by Pereira et al. and their published results. After emailing the author, I was able to confirm that the only difference in the two implementations were the heuristics used for selecting the training patches, which were not specified in the paper published by Pereira. Hence, if I had to do this project again, I would choose to replicate a more detailed paper or a paper for which the source code is available. This difference in results also shows the importance of the quality of the input data for deep learning methods, which is often more important than the architectural details of the network. Interestingly, increasing the receptive field of the neural network to a larger patch surrounding the voxels to be classified did not significantly increase the performance of the network. This indicates that the class of a voxel is highly dependent on the surrounding voxels, but not on those voxels slightly further apart.

\section{Future work}
A likely future direction is to use three-dimensional patches, instead of being restricted to two-dimensional patches. Incorporating the third dimension is challenging in convolutional neural networks due to the exponential growth in parameters associated with it. However, some very recent approaches have overcome these difficulties and use \mbox{three-dimensional} patches to achieve state-of-the-art results \cite{kamnitas}. 

A second likely future direction concerns the fact that convolutional neural networks are not able to give any measure of uncertainty for their predictions, which might be very important for life-or-death cases such as brain tumours. Some recent work by Gal and Ghahramani showed how it is possible to get such uncertainty bounds in convolutional neural networks using Dropout \cite{Gal2015Dropout}. It would be interesting to research how this new knowledge could be incorporated into models for brain tumour segmentation.

%TC:ignore

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% the appendices
\appendix

%\chapter{Latex source}
%
%\section{diss.tex}
%{\scriptsize\verbatiminput{diss.tex}}
%
%\section{proposal.tex}
%{\scriptsize\verbatiminput{proposal.tex}}

\chapter{Dropout}
\label{appendix:dropout}
A more recent technique, developed for deep neural networks is Dropout \cite{dropout}. When Dropout is used, the network randomly drops some neurons during the training phase. This forces the network to distribute the computation on all neurons evenly, which prevents the network from overfitting. The computation done by a single neuron thus becomes
\begin{equation}
\begin{split}
	\textbf{r} & \sim \text{Bernoulli}_n(p) \\
	y & = f_{act}((\sum_{i=1}^{n} r_i y_i w_i) + b)
\end{split}
\end{equation}
where $\textbf{r}$ is a vector of $n$ independent Bernoulli random variables each with parameter $p$, that is $P(r_i = 1) = p$ and $P(r_i = 0) = 1 - p$ for each $r_i$. This is done at every layer and therefore amounts to sampling a sub-network from a larger network. 

At test time dropout is not applied. Thus, the network weights have to be scaled by a factor $p$ to compensate for the extra inputs at each layer. The back-propagation algorithm remains unchanged for the sub-network and can therefore be applied in the learning phase.

In practice, it is common to apply Dropout only to the fully connected layers and not to the convolutional layers, as it has been shown to produce better results. As I will show later, this is also what was done by Pereira \cite{pereira}.

\chapter{Batch normalisation}
\label{appendix:batch_normalisation}
The key realisation behind the batch normalisation algorithm \cite{batch_normalization} is that the distribution of the inputs to each layer of the network changes as the weights are changed. This phenomenon is called \textit{internal covariance shift}. The training of the neural network is slowed down by this because each layer has to continuously adapt to this change in the distribution of its inputs. By fixing the distribution of these inputs, the network is able to learn faster and is less prone to overfitting. Similarly to how the inputs to the network are often normalised to have mean 0 and variance 1, it would be beneficial to ensure that the input vector $\textbf{x}$ to each layer has mean 0 and variance 1. 

To make this efficient and differentiable, which is required for the minimisation of the loss function, each individual dimension of the input vector $\textbf{x}^{(k)}$ is normalised using the mean and standard deviation of the training mini-batch. In the same way as the mini-batch is used as an approximation to calculate the gradient of the loss function on the entire training set, the mini-batch is used to approximate the mean and variance of the entire training set at different layers in the network. However, just normalising each input of a layer might restrict which functions the layer is able to represent. The output is therefore linearly scaled by $\gamma$ and shifted by $\beta$, parameters that can be learned along with the weights. This ensures that the introduced transformation is able to represent the identity transformation, if that is the optimal thing to do. The Batch Normalisation transformation computes:
\begin{equation}
\begin{split}
	\mu_{\mathbb{B}}  & = \frac{1}{m}\sum_{i=1}^m x^{(k)}_i \\	
	\sigma^2_{\mathbb{B}}  & = \frac{1}{m}\sum_{i=1}^m (x^{(k)}_i - \mu_{\mathbb{B}})^2 \\	
	\hat{x}^{(k)}_i & = \cfrac{x^{(k)}_i - \mu_{\mathbb{B}}}{\sqrt{\sigma^2_{\mathbb{B}} + \epsilon}} \\
	\textrm{BN}_{\gamma, \beta}(x^{(k)}_i) & = \gamma \hat{x}^{(k)}_i + \beta
\end{split}
\end{equation}
where $\mathbb{B}$ is a mini-batch of size $m$, $\mathbb{B} = \{ \textbf{x}_1, ... , \textbf{x}_m \}$ and $\epsilon$ is a numerical constant added to increase numerical stability.
In convolutional neural networks, the Batch Normalisation transformation is applied just before the activation function. The output computed by each neuron therefore becomes
\begin{equation}
		y = f_{act}(\textrm{BN}_{\gamma, \beta}(\sum_{i=1}^{n} y_i w_i))
\end{equation}
where the Batch Normalisation transformation is applied to each dimension individually. Notice that since the parameter $\beta$ is added, the bias weight $b$ is made redundant. The testing phase has to be modified accordingly, the details of which can be found in \cite{batch_normalization}.

\chapter[Implementation of the model proposed by Pereira]{Python implementation of the model proposed by Pereira with Keras}
\label{appendix:pereira_model}

\lstinputlisting[style=python, language=Python]{../src/model/model.py}

\chapter{Python implementation of my model with Keras}
\label{appendix:my_model}
\lstinputlisting[style=python, language=Python]{../src/model/fcn_model.py}

\chapter{Email correspondence with Pereira}
\label{appendix:pereira_email}
\includegraphics[width=\textwidth]{email2.pdf}
\includegraphics[width=\textwidth]{email2.pdf}

\chapter{Project Proposal}

\input{proposal}
%TC:endignore

\end{document}
